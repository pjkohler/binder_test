{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import some packages \n",
    "As in often the case, we start our code by importing some Python modules. \n",
    "\n",
    "Remember: **Your code will not work** unless you run the cell in which the modules are imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00000-3df10703-604d-4e4d-b7e4-0c48cb7e45da",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 6318,
    "execution_start": 1643303697123,
    "source_hash": "b97f439c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import python modules to use for analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from IPython.utils import io\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# install package for fitting\n",
    "install_psignifit = True\n",
    "\n",
    "if install_psignifit:\n",
    "    # stuff for fitting pphys\n",
    "    # insert at 1, 0 is the script path (or '' in REPL)\n",
    "    with io.capture_output() as temp:\n",
    "        !pip install https://github.com/wichmann-lab/python-psignifit/zipball/master\n",
    "\n",
    "import psignifit as ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define some functions\n",
    "In the cell below we define two functions\n",
    "    \n",
    "    fit_ps\n",
    "    \n",
    "    plot_ps\n",
    "\n",
    "that will fit fit the psychometric function and plot the data, respectively. You do not need to worry about the internal workings of those functions, although you are welcome to explore them if you are interested. \n",
    "\n",
    "Remember: **Your code will not work** unless you run the cell in which the functions are defined first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ps(stim_params, fit_data, n_trials, options=dict()):\n",
    "    fit_data = np.vstack([np.array(stim_params), np.array(fit_data)*np.array(n_trials), np.array(n_trials)])\n",
    "    if not options:\n",
    "        options = dict();   # initialize as an empty dictionary\n",
    "        options['sigmoidName'] = 'norm';   # choose a cumulative Gauss as the sigmoid  \n",
    "        options['expType']     = 'equalAsymptote';   # choose 2-AFC as the experiment type  \n",
    "                                   # this sets the guessing rate to .5 (fixed) and  \n",
    "                                   # fits the rest of the parameters\n",
    "    with io.capture_output() as captured:\n",
    "        temp_params = ps.psignifit(fit_data.transpose(),options)\n",
    "    threshold = ps.getThreshold(temp_params,.5)[0]\n",
    "    slope = ps.getSlopePC(temp_params,.5)\n",
    "    return temp_params, threshold, slope\n",
    "\n",
    "def plot_ps(info_df, fit_params, participant, annotate=False, plot_function=False):\n",
    "    # plot data from two experiments\n",
    "    condition_list = [\"same\", \"small\"]\n",
    "    \n",
    "    if not participant in [\"average\", \"averages\", \"mean\", \"means\"]:\n",
    "        if participant in info_df['ID'].to_list():\n",
    "            cur_df = info_df.loc[info_df['ID'] == participant]\n",
    "            idx = info_df[info_df['ID'] == participant].index[0]\n",
    "            cur_params = [ x[idx-1] for x in fit_params ] \n",
    "            # use pse and slope from individual subject\n",
    "            cur_pse = [cur_df[\"pse-{0}\".format(c)].to_list()[0] for c in condition_list ]\n",
    "            cur_slope = [cur_df[\"slope-{0}\".format(c)].to_list()[0] for c in condition_list ]\n",
    "        else:\n",
    "            print(\"Unknown participant! Try again!\")\n",
    "            return\n",
    "    else:\n",
    "        cur_df = info_df\n",
    "        cur_params = [ x[-1] for x in fit_params ]\n",
    "        # use pse and slope from average fit\n",
    "        cur_pse = [cur_df[\"pse-{0}-avefit\".format(c)].to_list()[0] for c in condition_list ]\n",
    "        cur_slope = [cur_df[\"slope-{0}-avefit\".format(c)].to_list()[0] for c in condition_list ]\n",
    "\n",
    "    ps_kwargs = dict(plotData=False, showImediate=False, plotAsymptote=False)\n",
    "    eb_kwargs = dict(linestyle='none', mfc='none', ms=8, clip_on=False)\n",
    "\n",
    "    fig, ax = plt.subplots(1,2, figsize=[15,5])\n",
    "    for t, data_type in enumerate([\"rt\", \"testbigger\"]):\n",
    "        for c, cond in enumerate(condition_list):\n",
    "            means = [cur_df[\"{0}-{1}-{2}\".format(data_type, cond, str(x))].mean() for x in test_inner_sizes]\n",
    "            stddev = [cur_df[\"{0}-{1}-{2}\".format(data_type, cond, str(x))].std() for x in test_inner_sizes]\n",
    "            n = [cur_df[\"{0}-{1}-{2}\".format(data_type, cond, str(x))].count() for x in test_inner_sizes]\n",
    "            stderr = [i / np.sqrt(j) for i, j in zip(stddev, n)]\n",
    "            ci = [x * 1.96 for x in stderr]\n",
    "            if cond is 'same':\n",
    "                eb_kwargs[\"marker\"]='o'\n",
    "            else:\n",
    "                eb_kwargs[\"marker\"]='s'\n",
    "            e_h = ax[t].errorbar(test_inner_sizes, means, yerr=ci, label=condition_list[c]+ \" inducers\", **eb_kwargs)\n",
    "            ax[t].set_xticks(test_inner_sizes)\n",
    "\n",
    "            if data_type == 'rt':\n",
    "                ax[t].set_ylim([0,5000])\n",
    "            else:\n",
    "                ax[t].set_ylim([0,1])\n",
    "            if \"testbigger\" in data_type:\n",
    "                title_str = 'Responses'\n",
    "                ylabel_str = \"test > ref\"\n",
    "                many_xx = np.linspace(20, 30)\n",
    "                if plot_function:\n",
    "                    ps.psigniplot.plotPsych(cur_params[c], axisHandle=ax[t],lineColor=e_h[0].get_color(), **ps_kwargs)\n",
    "                ax[t].plot((10, 40), (0.5, 0.5), 'k:')\n",
    "                \n",
    "                if annotate:\n",
    "                    if cond is \"same\":\n",
    "                        ax[t].text(26, 0.40, \"PSE = {:.2f}, slope = {:.2f}\".format(cur_pse[c], cur_slope[c]), color=e_h[0].get_color())\n",
    "                    else:\n",
    "                        ax[t].text(26, 0.45, \"PSE = {:.2f}, slope = {:.2f}\".format(cur_pse[c], cur_slope[c]), color=e_h[0].get_color())\n",
    "            else:\n",
    "                title_str = 'Reaction Time'\n",
    "                ylabel_str = \"RT (ms)\"\n",
    "            ax[t].set_xlim([19,31])  \n",
    "\n",
    "            ax[t].set_xlabel(\"inner test size (ref: 25)\")\n",
    "            ax[t].set_ylabel(ylabel_str)\n",
    "            ax[t].title.set_text(title_str)\n",
    "            ax[t].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Get the data from Open Science Foundation\n",
    "The data files are shared in an public repository at the [Open Science Foundation](https://osf.io/s6mxd/]). The code below simply grabs the data and makes it accessible to the current workbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!osf --project s6mxd clone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load and organize the data from each participant\n",
    "\n",
    "The code below loads in the individual data files in csv format that each participant, including you, created when they did the experiment. When running an experiment online, you would normally have some sort of back-end that saves the files automatically somewhere, so you do not have to trust your participant to send you the raw data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00001-1b1b72db-540a-4bc0-9f0b-9cadc7d133af",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2210,
    "execution_start": 1643303894254,
    "source_hash": "ac864561",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = [\"s6mxd/osfstorage/data\"]\n",
    "\n",
    "# define empty lists that will hold the number of subjects, rejected subjects and test subjects\n",
    "sub_count = [0]*len(data_dir)    # included datasets\n",
    "reject_count = [0]*len(data_dir) # complete datasets, but rejected due to performance\n",
    "test_count = [0]*len(data_dir)   # incomplete test datasets\n",
    "\n",
    "complete_subs = [] # subjects that produced complete datasets\n",
    "\n",
    "prac_n = 16 # number of practice trials\n",
    "exp_n = 160 # number of experiment trials\n",
    "\n",
    "info_df = pd.DataFrame()\n",
    "\n",
    "for e, cur_dir in enumerate(data_dir):\n",
    "    file_list = glob.glob(cur_dir + \"/**/psyc4260*.csv\") + glob.glob(cur_dir + \"/**/nrsc2200*.csv\")\n",
    "    file_list.sort()\n",
    "    exp_subs = [] # list to hold the subjects in this experiment\n",
    "    for file in file_list:\n",
    "        # load the data\n",
    "        try:\n",
    "            sub_data = pd.read_csv(file)\n",
    "            if \"trial_type\" not in sub_data:\n",
    "                sub_data = pd.read_csv(file, skiprows=1)\n",
    "            # in the past, some trials were duplicated in the data file, the code below takes care of that\n",
    "            sub_data = sub_data[sub_data['trial_index'].apply(lambda x: str(x).isdigit())]\n",
    "            sub_data = sub_data.drop_duplicates()\n",
    "        except:\n",
    "            print(\"Failed to load file {0}\".format(file.split(cur_dir)[1]))\n",
    "            \n",
    "        # get id\n",
    "        try:\n",
    "            survey_resp = sub_data[sub_data[\"trial_type\"]==\"survey-html-form\"].loc[0][\"response\"]\n",
    "            survey_resp = survey_resp.replace(':\"}',':\"\"}')\n",
    "            sub_info = ast.literal_eval(survey_resp)\n",
    "        except:\n",
    "            sub_info = {}\n",
    "        # see if id was stored\n",
    "        if 'p_id' in sub_info.keys():\n",
    "            # if participant ID was recorded, use it to get rid of data with test subject ID 214984\n",
    "            # this is the ID associated with URPP's sample link\n",
    "            try:    \n",
    "                sub_id = sub_info[\"p_id\"]\n",
    "            except:\n",
    "                sub_id = \"nan\"\n",
    "            del sub_info['p_id']\n",
    "            if sub_id in [\"214984\", \"603921b616b3893be91674d1\"]:\n",
    "                test_count[e] = test_count[e] + 1\n",
    "                continue\n",
    "        else:\n",
    "            sub_id = \"nan\"\n",
    "        # do quality control on the data\n",
    "        if sub_data.shape[1] < 10:\n",
    "            print(e, sub_id, \"incomplete file, shape:{0}x{1}\".format(sub_data.shape[0],sub_data.shape[1]))\n",
    "            test_count[e] = test_count[e] + 1 # record test subject to the test_count, by adding 1 at the relevant position\n",
    "            continue\n",
    "        \n",
    "        # now skip if this subjects data is already in the set\n",
    "        if sub_id in exp_subs:\n",
    "            continue\n",
    "        \n",
    "        # add participant to list of subjects for this experiment\n",
    "        exp_subs.append(sub_id)\n",
    "        \n",
    "        # start populating sub_info dict: \n",
    "        sub_info[\"experiment\"] = e\n",
    "        sub_info[\"ID\"] = sub_id\n",
    "        \n",
    "        # get course and timing for this subject\n",
    "        course = file.split('-')[0].split('/')[-1].upper()\n",
    "        \n",
    "        try:\n",
    "            dt = datetime.strptime(sub_data[\"recorded_at\"][0], '%Y-%m-%d %H:%M:%S')\n",
    "        except:\n",
    "            dt = datetime.strptime(time.ctime(os.path.getmtime(file)), '%a %b %d %H:%M:%S %Y')\n",
    "            \n",
    "        if dt.month < 5:\n",
    "            sub_info[\"course\"] = \"{}_W{}\".format(course, dt.year)\n",
    "        elif dt.month > 8:\n",
    "            sub_info[\"course\"] = \"{}_F{}\".format(course, dt.year)\n",
    "        else:\n",
    "            sub_info[\"course\"] = \"{}_S{}\".format(course, dt.year)\n",
    "\n",
    "        # Cognition.run specific error: replace strings with logicals\n",
    "        sub_data = sub_data.replace(\"true\", True)\n",
    "        sub_data = sub_data.replace(\"false\", False)\n",
    "        \n",
    "        pphys_data = sub_data[sub_data[\"trial_type\"] == \"psychophysics\"]\n",
    "        pphys_data = pphys_data.replace(np.nan, \"\")\n",
    "        pphys_data = pphys_data.replace('\"', \"\")\n",
    "        pphys_data = pphys_data[pphys_data.test_inner_radius != \"\"]\n",
    "        pphys_data = pphys_data.astype(\n",
    "            {\"test_inner_radius\": 'int', \"test_outer_radius\": 'int', \"ref_inner_radius\": 'int', \"ref_outer_radius\": 'int' })\n",
    "        \n",
    "        test_inner_sizes = [x for x in pphys_data[\"test_inner_radius\"].unique()]\n",
    "        test_inner_sizes.sort()\n",
    "        \n",
    "        test_outer_sizes = [x for x in pphys_data[\"test_outer_radius\"].unique()]\n",
    "        test_outer_sizes.sort()\n",
    "        \n",
    "        ref_outer_size = [x for x in pphys_data[\"ref_outer_radius\"].unique()][0]\n",
    "        ref_inner_size = [x for x in pphys_data[\"ref_inner_radius\"].unique()][0]\n",
    "        \n",
    "        rt_reject = False\n",
    "        acc_reject = False\n",
    "        for size in test_inner_sizes:\n",
    "            for o in test_outer_sizes:\n",
    "                cur_data = pphys_data[(pphys_data[\"test_inner_radius\"]==size) & (pphys_data[\"test_outer_radius\"]==o)]\n",
    "                # only include trials with RTs less than 5 secs\n",
    "                cur_data = cur_data[cur_data[\"rt\"] < 3000]\n",
    "                if len(cur_data) < 10:\n",
    "                    rt_reject = True\n",
    "                if o == ref_outer_size:\n",
    "                    sub_info[\"rt-same-\"+str(size)] = cur_data[\"rt\"].mean()\n",
    "                    temp_bigger = cur_data[\"test_bigger\"]\n",
    "                    sub_info[\"testbigger-same-\" + str(size) ] = np.nansum(temp_bigger)/len(temp_bigger)\n",
    "                    sub_info[\"ntrials-same-\" + str(size) ] = len(temp_bigger)\n",
    "                    if size == test_inner_sizes[0]:\n",
    "                        if np.nansum(temp_bigger)/len(temp_bigger) > 0.2:\n",
    "                            acc_reject = True\n",
    "                    if size == test_inner_sizes[-1]:\n",
    "                        if np.nansum(temp_bigger)/len(temp_bigger) < 0.8:\n",
    "                            acc_reject = True\n",
    "                else:\n",
    "                    sub_info[\"rt-small-\"+str(size)] = cur_data[\"rt\"].mean()\n",
    "                    temp_bigger = cur_data[\"test_bigger\"]\n",
    "                    sub_info[\"testbigger-small-\" + str(size) ] = np.nansum(temp_bigger)/len(temp_bigger)\n",
    "                    sub_info[\"ntrials-small-\" + str(size) ] = len(temp_bigger)\n",
    "        if acc_reject:\n",
    "            print(\"Excluding {} : {}, chance performance\".format(sub_info[\"course\"], sub_id))\n",
    "            continue \n",
    "        elif rt_reject:\n",
    "            print(\"Excluding {} : {}, less than 10 trials with RT < 3 secs\".format(sub_info[\"course\"], sub_id))\n",
    "            continue\n",
    "        sub_count[e] = sub_count[e]+1\n",
    "        sub_info[\"count\"] = sub_count[e]\n",
    "        info_df = pd.concat([info_df, pd.json_normalize(sub_info)], sort=False)\n",
    "\n",
    "        # add to dataframe of complete subjects \n",
    "        complete_subs.append(exp_subs)\n",
    "info_df = info_df.set_index(\"count\")\n",
    "\n",
    "cols = info_df.columns.tolist()\n",
    "cols.sort()\n",
    "cols.insert(0, cols.pop(cols.index(\"experiment\")))\n",
    "if \"age\" in cols:\n",
    "    cols.insert(2, cols.pop(cols.index(\"age\")))\n",
    "    cols.insert(3, cols.pop(cols.index(\"handedness\")))\n",
    "    cols.insert(4, cols.pop(cols.index(\"sex\")))\n",
    "    cols.insert(5, cols.pop(cols.index(\"other_sex\")))\n",
    "info_df = info_df[cols]\n",
    "info_df = info_df.sort_values(by = [\"experiment\", \"course\", \"ID\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Fit psychometric functions to the data\n",
    "\n",
    "This code uses the function\n",
    "    \n",
    "    fit_ps\n",
    "    \n",
    "so you will be in trouble if you have not run the cell above in which this function is defined. Fitting the psychometric functions takes a few minutes. \n",
    "\n",
    "After fitting, the data from all participants are stored in a variable type called a *Pandas dataframe*. This variable type is useful in many ways, including that it makes it easy to save the combined data in a new csv file that can be used statistical analysis or figure making outside of Python. You will not be working with this new csv file in this course, instead you will be grabbing the data directly from the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00004-4cec8dc3-82a6-44a6-a86f-7327b0a3e944",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 90378,
    "execution_start": 1643303743125,
    "source_hash": "418f07e5"
   },
   "outputs": [],
   "source": [
    "fit_params = [ [], [] ]\n",
    "pse_slope = [ [], [] ]\n",
    "for d, data_type in enumerate([\"same\", \"small\"]):\n",
    "    print('Fitting psychometric functions for \"{}\" condition ... '.format(data_type), end='')\n",
    "    for index, row in info_df.iterrows():\n",
    "        # same inducers\n",
    "        fit_data = [ row[x] for x in info_df.columns if x.startswith(\"testbigger-{0}-\".format(data_type)) ]\n",
    "        n_trials = [ row[x] for x in info_df.columns if x.startswith(\"ntrials-{0}-\".format(data_type)) ]\n",
    "        temp_params, threshold, slope = fit_ps(test_inner_sizes, fit_data, n_trials)\n",
    "        fit_params[d].append(temp_params)\n",
    "        pse_slope[d].append((threshold, slope))\n",
    "    # run fit on averages for illustration purposes\n",
    "    fit_data = [ info_df[x].mean() for x in info_df.columns if x.startswith(\"testbigger-{0}-\".format(data_type)) ]\n",
    "    n_trials = [ int(info_df[x].mean()) for x in info_df.columns if x.startswith(\"ntrials-{0}-\".format(data_type)) ]\n",
    "    temp_params, threshold, slope = fit_ps(test_inner_sizes, fit_data, n_trials)\n",
    "    fit_params[d].append(temp_params)\n",
    "    pse_slope[d].append((threshold, slope))\n",
    "    # assign pses to info_df\n",
    "    info_df[\"pse-{0}\".format(data_type)] = [ x[0] for x in pse_slope[d][0:-1] ]\n",
    "    info_df[\"pse-{0}-avefit\".format(data_type)] = [pse_slope[d][-1][0]] * info_df.shape[0]\n",
    "    info_df[\"slope-{0}\".format(data_type)] = [ x[1] for x in pse_slope[d][0:-1] ]\n",
    "    info_df[\"slope-{0}-avefit\".format(data_type)] = [pse_slope[d][-1][1]] * info_df.shape[0]\n",
    "    print('finished!'.format(data_type))\n",
    "# save data to a csv\n",
    "info_df.to_csv(\"ebbinghaus_combined.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-bd8e3025-eef9-45ac-9a0d-88937cf97e97",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 6,
    "execution_start": 1643303863757,
    "source_hash": "4b09fed"
   },
   "source": [
    "## Assignment 2\n",
    "### Question 1 (4 pts): \n",
    "\n",
    "The code \n",
    "\n",
    "    plot_ps(info_df, fit_params, participant_id)\n",
    "    \n",
    "creates two plots, one of the reaction time and one of the responses. The input arguments info_df and fit_params should already be on the workspace if you have run steps 1-5 of the code, and the argument participant_id is a string that indicates the participant ID you want to plot. You can also pass \"means\" to participant_id and the function will plot the average across all participants.\n",
    "\n",
    "Reaction times and responses are plotted seperately for each of the seven physical sizes used for the inner *test disc* (in pixels): 20, 23, 24, 25, 26, 27, 30. The physical size of the inner *reference disc* was always 25 pixels. The same inducers condition is shown in blue, and the small inducers in orange. \n",
    "\n",
    "(a) Please use the function plot_ps to plot the averages across all participants. \n",
    "\n",
    "(b) Then use the function plot_ps to plot your own data - input your own participant ID to the function. If your data was excluded you may plot someone else's ID. Participant \"999\" has very reasonable data. \n",
    "\n",
    "(c) Then draw, by hand, S-shaped *Psychometric functions* through the data, and indicate the approximate *Point of Subjective Equality* (PSE). Do this separately for the same and small inducer conditions, in different colors. Do this for both the average data and your own data.  \n",
    "\n",
    "(d) Based on what you did in 1a-1c, is your effect size, measured using the PSEs, bigger than the average or smaller than the average? Explain why. \n",
    "\n",
    "Your answer should include the code used in a and b (2 lines per question, at most) and screenshots of the *Responses* part of the plot with your hand drawn functions.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work on your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (4 pts): \n",
    "\n",
    "The code \n",
    "\n",
    "    pse_data = np.array(info_df[[\"pse-same\",\"pse-small\"]])\n",
    "\n",
    "grabs data from the Pandas dataframe to create a 2 x n numpy array that contains the PSEs for the same and small inducers condition, respectively. \n",
    "\n",
    "(a) Please use the *shape* method to get the **number of participants in your dataset** and save that as a integer variable named\n",
    "    \n",
    "    num_subs\n",
    "\n",
    "(b) Please use the methods *mean* and *std* or their corresponding numpy commands to compute the **mean** and **standard deviation**, seperately for the two conditions, and save them as two variables named:\n",
    "\n",
    "    pse_means\n",
    "    pse_stdev\n",
    "\n",
    "Note that both mean and standard deviation can be computed in one line of code.\n",
    "\n",
    "(c) Watch this [video](https://www.youtube.com/watch?v=AQy11Hfp_dU) (also on eClass) to learn how to compute **standard error** using the sample size, and how to convert standard error to the 95% confidence interval. You can use the command *np.sqrt* to take the square root of a number. Save as two variables named:\n",
    "    \n",
    "    pse_stderr # standard error\n",
    "    pse_ci # interval\n",
    "\n",
    "Use the following code to plot your data as a bar plot with error bars:\n",
    "    \n",
    "    plt.bar((0,1), pse_means, yerr=pse_ci, capsize=5)\n",
    "    plt.ylim([15,30])\n",
    "    \n",
    "Try replacing pse_ci with pse_stderr in the above and observe how the error bars change. \n",
    "    \n",
    "(d) The size of the *Ebbinghaus effect* for each participant can be computed as the difference between pse_same and pse_small. Use pse_data to subtract pse_small from pse_same and assign the output to a new variable called:\n",
    "\n",
    "    pse_diff\n",
    "\n",
    "this can be done in one line by selecting the different columns of pse_data. Now use the method *max* or its corresponding numpy command to assign the maximum effect size to a new variable called\n",
    "\n",
    "    pse_max\n",
    "    \n",
    "Bonus: Use the method *argmax* or its corresponding numpy command to get the index of the participant that has the maximum effect size. You can then get the ID of that participant using this command\n",
    "    \n",
    "    print(np.array(info_df[[\"ID\"]])[max_id,0])\n",
    "\n",
    "where max_id is the index. \n",
    "\n",
    "Your answer should include the code used in a-d and a screenshot of the bar plot you created in (c). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work on your answer here:\n",
    "pse_data = np.array(info_df[[\"pse-same\",\"pse-small\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (2 pts):\n",
    "\n",
    "According to the article “The surface area of human V1 predicts the subjective experience of object size”, linked on eClass, what would you expect to be true about primary visual cortex (area V1) of those participants who have the largest Ebbinghaus effects in your experiment?"
   ]
  }
 ],
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "cb230051-44ac-4368-ae81-c257a4783978",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
